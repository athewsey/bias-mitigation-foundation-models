{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18febc2a-572f-4bd1-aa4b-2b1669612bdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1 - Bias mitigation in text based foundation models using prompt engineering\n",
    "\n",
    "references: https://medium.com/engineering-at-eightfold-ai/mitigating-bias-integrating-generative-ai-foundation-models-and-llms-in-enterprise-workflows-eda62a15f376"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce169a10-ae85-4a2a-b6d2-8bed3cc89b2f",
   "metadata": {},
   "source": [
    "Use kernel `Data Science 3.0` and instance `ml.t3.medium` with `2vCPU + 4GiB` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea9cac-59c2-4931-9cea-c8827ac06629",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install 'stability-sdk[sagemaker] @ git+https://github.com/Stability-AI/stability-sdk.git@sagemaker' --quiet\n",
    "!pip install protobuf==3.20 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016cd3a-2d01-47d3-9678-af17a53bc4f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain \n",
    "# !transformers==4.24.0 sentence_transformers==2.2.2 ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c2d68-c51f-4116-af30-a32284517577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain \n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9b145-1f55-460a-804b-d4a3ed91c66a",
   "metadata": {},
   "source": [
    "*Note: Restart kernel after installing the packages.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113354ac-af4d-4f5c-be8c-87fa08d2aaf0",
   "metadata": {},
   "source": [
    "## Examples of bias in text-text models and techniques to mitigate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf567e-75d8-4924-b8a4-c119855f9f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8862239-cd66-444f-8df0-741eaf79f7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r falcon_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a99755-6f91-4765-9c32-5da9b2b962c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"FALCON_7B_ENDPOINT\"]=falcon_endpoint_name\n",
    "print(os.environ[\"FALCON_7B_ENDPOINT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1705be1-a0bf-4b35-ae77-bb9a8f40b498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "role = sagemaker.get_execution_role() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75e131-b857-4ddb-bfc2-6e4793832651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# falcon 7B parameters\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "NUM_RETURN_SEQUENCES = 1\n",
    "TOP_K = 0\n",
    "TOP_P = 0.9\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fac1f-237d-4b29-9f4b-b7303fe79b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# falcon parameters\n",
    "falcon_parameters = {\n",
    "            \"do_sample\": DO_SAMPLE,\n",
    "            \"top_p\": TOP_P,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"max_new_tokens\": MAX_LENGTH,\n",
    "            \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19410514-b11f-49e5-8f0d-39b2f97fd9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_kwargs = {}\n",
    "session = boto3.Session()\n",
    "\n",
    "boto3_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "print(boto3_sm_client)\n",
    "\n",
    "def invoke_sagemaker_model_falcon(prompt):\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": falcon_parameters\n",
    "    }\n",
    "    payload = json.dumps(payload).encode('utf-8')\n",
    "    response = boto3_sm_client.invoke_endpoint(\n",
    "        EndpointName=os.environ[\"FALCON_7B_ENDPOINT\"],\n",
    "        Body=payload,\n",
    "        ContentType=\"application/json\",\n",
    "    )[\"Body\"].read().decode(\"utf8\")\n",
    "    return json.loads(response)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cac0cf-7be8-41eb-a15f-4437e5367377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"How can you help me? \"\n",
    "print(invoke_sagemaker_model_falcon(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a621e-4036-4ee6-8caa-3d04d25b9154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biased_prompt = f\"Write a python program to determine if a person is a good programmer based on the university they studied\"\n",
    "response = invoke_sagemaker_model_falcon(biased_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce327bb9-b428-4235-b6bd-d9a2adbcba62",
   "metadata": {},
   "source": [
    "The above program is biased as its not possible to determine is the person will be a good scientist, in fact the prompt is biased in itself. If we update the prompt the foundation model should be able to respond in a responsible way. Therefore, to mitigate this bias, bias inducing content should be removed from the prompt. For this example, this means removing emphasis on “based on the college they attended”. With this change, the models will not focus on a biased factor such as the college the scientist attended and instead they will focus on non-biased factors to determine whether someone is a good scientist or not (assuming the model itself is not biased in this context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ec8a7-f2f4-43f6-9128-6eea44738309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enhanced_prompt = f\"Instructions: Answer the following question, and make sure that the response is not biased ad does not discriminate against certain groups of people. If response is discriminatory or bias say ' I am a responsible AI model and cannot answer the question as its discriminatory and bias in nature.' \\\n",
    "question: 'Write a python program to determine if the person will be good data scientist based on the university they attend.\\\n",
    "answer:\"\n",
    "response = invoke_sagemaker_model_falcon(enhanced_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c855aa-8a54-435f-98c7-69892ea10a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enhanced_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619f784-99bc-420d-ab0e-f26688433608",
   "metadata": {},
   "source": [
    "### Automating the prompts for mitigating bias using prompt templates from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1012a9-2996-491b-aceb-4abc1143c579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import LLMChain\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151ced3-3fce-483c-bf43-df2e89d53496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        prompt = prompt[:1024]\n",
    "\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler_sm_llm = ContentHandler()\n",
    "session = boto3.Session()\n",
    "boto3_sm_client = boto3.client(\n",
    "    \"sagemaker-runtime\"\n",
    "    # **boto3_kwargs\n",
    ")\n",
    "print(boto3_sm_client)\n",
    "\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    client = boto3_sm_client,\n",
    "    endpoint_name=os.environ[\"FALCON_7B_ENDPOINT\"],\n",
    "    region_name='us-east-1',\n",
    "    model_kwargs=falcon_parameters,\n",
    "    content_handler=content_handler_sm_llm,\n",
    ")\n",
    "\n",
    "print(f\"SageMaker LLM created at {sm_llm}::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fa32b-c64e-4089-aac3-0d470428dbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Instructions: Answer the following question, and make sure that the response is not biased ad does not discriminate against certain groups of people.\\ \n",
    "If response is discriminatory or bias say ' I am a responsible AI model and cannot answer the question as its discriminatory and bias in nature.'\\\n",
    "question: \"{question}\"\\\n",
    "answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89295f-97cd-4294-8bf3-eeb2485fdc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=sm_llm,\n",
    "    prompt=PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77603a0-aef8-4635-9fd1-8cabd2f5d5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Write a python program to determine if the person will be good data scientist based on the university they attend\"\"\"\n",
    "chain({\"question\":query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35227b5e-b7da-4833-b21d-3a1b20789746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Write an essay on Doctor.\"\"\"\n",
    "chain({\"question\":query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284e826-3837-4a0d-a46c-f5b9950b0eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Write an essay on a doctor, without referring to the gender.\"\"\"\n",
    "chain({\"question\":query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253670f-61d8-4427-a195-78dddfd2ba75",
   "metadata": {},
   "source": [
    "#### Summary for mitigating bias\n",
    "In the above example the issue was in the prompt itself. So there are 3 ways of mitigating bias: \n",
    "- by removing the bias inducing content from the prompt\n",
    "- adding instructions to make sure that the model focus on the providing the response which is non-discriminatory. \n",
    "- automating it using prompt templates, which can be integrated with your virtual assistant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e0e29-1a34-4aed-a88d-7f5df55f39be",
   "metadata": {},
   "source": [
    "### Partial question in a prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cae3f-d5ed-49a1-abe0-84e71ebb8567",
   "metadata": {},
   "source": [
    "Look at the below question, where the model is making an assumption that insulin shots are needed by all diabetic patients and also that it is an effective treatment option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ed787-5eef-4307-9988-1f44b138dbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_question = \"Explain why insulin shots are an effective treatment for diabetes?\"\n",
    "response =invoke_sagemaker_model_falcon(partial_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959776a3-5160-4021-aed0-58ed680aaf75",
   "metadata": {},
   "source": [
    "The above response clearly highlights the assumption made by the model induced in the prompt and the sentiment towards using insulin shots for treating diabetes is very positive.\n",
    "Now we are modifying the question and removing the assumption and also providing the prompt in a neutral tone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522d837-1521-4353-bd1a-4fa4818aeaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_assumption_question = \"Is insulin shots an effective treatment for diabetes?\"\n",
    "response = invoke_sagemaker_model_falcon(no_assumption_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fa9ed-969e-42f0-a541-ed3e755f1964",
   "metadata": {},
   "source": [
    "Now, you would notice that the response is mostly positive in terms of sentiment, but the model is hestitant to make a claim that its an effective treament for diabetes. The output clearly highlights that although its a common treatment, it might not be the suitable for everyone. Moreover, mentions to consult the healthcare provider for best treatment plan. \n",
    "Therefore, its important to ask question in a clear manner without making any assumptions. \n",
    "Now lets take a look at the third prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf6a02-d28b-4e30-aa23-fe5895966ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benefits_drawbacks_question = \"What are the benefits and drawbacks of using insulin shots for treating diabetes?\"\n",
    "response = invoke_sagemaker_model_falcon(benefits_drawbacks_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3370ff-3b02-4c16-84fa-9a0d50978d11",
   "metadata": {},
   "source": [
    "The above response is neutral tone, and focuses on providing both the benefits and drawbacks of using insulin shots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d78dc-ac36-4089-8840-9c7444d36492",
   "metadata": {},
   "source": [
    "This type of issue is not something we are unfamiliar with. Many other technologies experience similar issues, for example if you use the same style of prompt as your search term on a search engine like Google you will see that the sentiment in the search results will be guided by the partiality in the search term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e1296-f18d-421b-9197-b1e9e5d27308",
   "metadata": {},
   "source": [
    "#### Summary for mitigating bias\n",
    "\n",
    "- add instructions in the prompt to mitigate bias.\n",
    "- remove bias inducing content in the prompt. \n",
    "- follow best practices such as \n",
    "    - avoid making assumptions\n",
    "    - encourage different perspectives such as the benefits and drawbacks. \n",
    "    - use open ended questions which helps models to explore different aspects of the content and helps to provide comprehensive analysis without generating response that fall to a particular bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042b8e5-bb32-4c83-b8c1-9f695a01a9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
