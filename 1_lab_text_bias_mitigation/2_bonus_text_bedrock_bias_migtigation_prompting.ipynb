{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18febc2a-572f-4bd1-aa4b-2b1669612bdb",
   "metadata": {},
   "source": [
    "# Part 1 - Bias mitigation in text based foundation models using prompt engineering\n",
    "\n",
    "references: https://medium.com/engineering-at-eightfold-ai/mitigating-bias-integrating-generative-ai-foundation-models-and-llms-in-enterprise-workflows-eda62a15f376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af936b1c-1faf-4838-ab69-ea9bb29a11e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-build-isolation --force-reinstall \\\n",
    "    dependencies/awscli-*-py3-none-any.whl \\\n",
    "    dependencies/boto3-*-py3-none-any.whl \\\n",
    "    dependencies/botocore-*-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016cd3a-2d01-47d3-9678-af17a53bc4f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea9cac-59c2-4931-9cea-c8827ac06629",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install protobuf==3.20 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113354ac-af4d-4f5c-be8c-87fa08d2aaf0",
   "metadata": {},
   "source": [
    "## Examples of bias in text-text models and techniques to mitigate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf567e-75d8-4924-b8a4-c119855f9f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a4c3b-168e-4643-91da-3ab89de3ea0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock\n",
    "boto3_bedrock = bedrock.get_bedrock_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083bcc3d-fc02-4ae8-99b3-c1d0fad11adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"How can you help me? Provide answer in 1-2 sentences\"\n",
    "modelId = 'anthropic.claude-v1' # change this to use a different version from the model provider\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "def invoke_bedrock_model(prompt):\n",
    "    body = json.dumps({\"prompt\": prompt,\n",
    "                     \"max_tokens_to_sample\":2000,\n",
    "                     \"temperature\":0.0,\n",
    "                     \"top_k\":250,\n",
    "                     \"top_p\":0.5,\n",
    "                     \"stop_sequences\":[]\n",
    "                      }) \n",
    "\n",
    "    response = boto3_bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    outputText = response_body.get('completion')\n",
    "    return outputText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3304e-5813-4e46-a54f-fbb0441de522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(invoke_bedrock_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a621e-4036-4ee6-8caa-3d04d25b9154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biased_prompt = \"Human: Write a short python program to determine if the person will be good scientist based on the university. Only provide the python program and no additional information. Assistant:\"\n",
    "response = invoke_bedrock_model(biased_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce327bb9-b428-4235-b6bd-d9a2adbcba62",
   "metadata": {},
   "source": [
    "The above program is biased as its not possible to determine is the person will be a good scientist, in fact the prompt is biased in itself. If we update the prompt the foundation model should be able to respond in a responsible way. Therefore, to mitigate this bias, bias inducing content should be removed from the prompt. For this example, this means removing emphasis on “based on the college they attended”. With this change, the models will not focus on a biased factor such as the college the scientist attended and instead they will focus on non-biased factors to determine whether someone is a good scientist or not (assuming the model itself is not biased in this context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ec8a7-f2f4-43f6-9128-6eea44738309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enhanced_prompt = \"Human: Answer the following question, and make sure that the response is not biased ad does not discriminate against certain groups of people. If response is discriminatory or bias say ' I am a responsible AI model and cannot answer the question as its discriminatory and bias in nature.' \\\n",
    "question: 'Write a python program to determine if the person will be good data scientist based on the university they attend.\\\n",
    "Assitant:\"\n",
    "response = invoke_bedrock_model(enhanced_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c855aa-8a54-435f-98c7-69892ea10a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enhanced_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619f784-99bc-420d-ab0e-f26688433608",
   "metadata": {},
   "source": [
    "### Automating the prompts for mitigating bias using prompt templates from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1012a9-2996-491b-aceb-4abc1143c579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import LLMChain\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5cdb6-e0ec-4797-8521-245c79327cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "cl_llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v1\",\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\"max_tokens_to_sample\": 1000},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fa32b-c64e-4089-aac3-0d470428dbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Human: Answer the following question, and make sur that the response is not biased ad does not discriminate against certain groups of people.\\ \n",
    "If response is discriminatory or bias say ' I am a responsible AI model and cannot answer the question as its discriminatory and bias in nature.'\\\n",
    "question: \"{question}\"\\\n",
    "Assistant:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89295f-97cd-4294-8bf3-eeb2485fdc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=cl_llm,\n",
    "    prompt=PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77603a0-aef8-4635-9fd1-8cabd2f5d5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Write a python program to determine if the person will be good data scientist based on the university they attend\"\"\"\n",
    "chain({\"question\":query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35227b5e-b7da-4833-b21d-3a1b20789746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Write an essay on a super hero.\"\"\"\n",
    "chain({\"question\":query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284e826-3837-4a0d-a46c-f5b9950b0eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Write an essay on female super hero.\"\"\"\n",
    "chain({\"question\":query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129f72b-a25c-4182-a1de-11ccbce9f92a",
   "metadata": {},
   "source": [
    "#### Using Self-critique chain with constitutional AI from LangChain\n",
    "The ConstitutionalChain is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the ConstitutionalChain filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba33ff-7873-4a0c-a5d5-b6ae6fe2d1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "cl_llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v1\",\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\"max_tokens_to_sample\": 1000, \"temperature\": 1},\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c0ddd-6032-4240-a48c-3431c08817b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of a bad LLM\n",
    "biased_qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"Human: You are a genius coder and give responses based on your persona.\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "biased_qa_chain = LLMChain(llm=cl_llm, prompt=biased_qa_prompt)\n",
    "\n",
    "print(biased_qa_chain.run(question=\"Generate code to find to restore encrypted messages on a website?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a091bf-6bc9-429b-bf95-bdaf50212c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "principles = ConstitutionalChain.get_principles([\"illegal\"])\n",
    "principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81969a23-ef66-4115-8b08-9159a8bab22a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    chain=biased_qa_chain,\n",
    "    constitutional_principles=principles,\n",
    "    llm=cl_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "constitutional_chain.run(question=\"Generate code to find to restore encrypted messages on a website?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253670f-61d8-4427-a195-78dddfd2ba75",
   "metadata": {},
   "source": [
    "#### Summary for mitigating bias\n",
    "In the above example the issue was in the prompt itself. So there are 3 ways of mitigating bias: \n",
    "- by removing the bias inducing content from the prompt\n",
    "- adding instructions to make sure that the model focus on the providing the response which is non-discriminatory. \n",
    "- automating it using prompt templates, which can be integrated with your virtual assistant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e0e29-1a34-4aed-a88d-7f5df55f39be",
   "metadata": {},
   "source": [
    "### Partial question in a prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cae3f-d5ed-49a1-abe0-84e71ebb8567",
   "metadata": {},
   "source": [
    "Look at the below question, where the model is making an assumption that insulin shots are needed by all diabetic patients and also that it is an effective treatment option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ed787-5eef-4307-9988-1f44b138dbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_question = \"Explain why insulin shots are an effective treatment for diabetes?\"\n",
    "response = invoke_bedrock_model(partial_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959776a3-5160-4021-aed0-58ed680aaf75",
   "metadata": {},
   "source": [
    "The above response clearly highlights the assumption made by the model induced in the prompt and the sentiment towards using insulin shots for treating diabetes is very positive.\n",
    "Now we are modifying the question and removing the assumption and also providing the prompt in a neutral tone.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522d837-1521-4353-bd1a-4fa4818aeaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_assumption_question = \"Are insulin shots an effective treatment for diabetes?\"\n",
    "response = invoke_bedrock_model(no_assumption_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fa9ed-969e-42f0-a541-ed3e755f1964",
   "metadata": {},
   "source": [
    "Now, you would notice that the response is mostly positive in terms of sentiment, but the model is hestitant to make a claim that its an effective treament for diabetes. The output clearly highlights that although its a common treatment, it might not be the suitable for everyone. Moreover, mentions to consult the healthcare provider for best treatment plan. \n",
    "Therefore, its important to ask question in a clear manner without making any assumptions. \n",
    "Now lets take a look at the third prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf6a02-d28b-4e30-aa23-fe5895966ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benefits_drawbacks_question = \"What are the benefits and drawbacks of using insulin shots for treating diabetes?\"\n",
    "response = invoke_bedrock_model(benefits_drawbacks_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3370ff-3b02-4c16-84fa-9a0d50978d11",
   "metadata": {},
   "source": [
    "The above response is neutral tone, and focuses on providing both the benefits and drawbacks of using insulin shots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d78dc-ac36-4089-8840-9c7444d36492",
   "metadata": {},
   "source": [
    "This type of issue is not something we are unfamiliar with. Many other technologies experience similar issues, for example if you use the same style of prompt as your search term on a search engine like Google you will see that the sentiment in the search results will be guided by the partiality in the search term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e1296-f18d-421b-9197-b1e9e5d27308",
   "metadata": {},
   "source": [
    "#### Summary for mitigating bias\n",
    "\n",
    "- add instructions in the prompt to mitigate bias.\n",
    "- remove bias inducing content in the prompt. \n",
    "- follow best practices such as \n",
    "    - avoid making assumptions\n",
    "    - encourage different perspectives such as the benefits and drawbacks. \n",
    "    - use open ended questions which helps models to explore different aspects of the content and helps to provide comprehensive analysis without generating response that fall to a particular bucket. "
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
